# 04. CONTEXT OVERFLOW HANDLING

## üìã T·ªïng quan

T√†i li·ªáu n√†y m√¥ t·∫£ **c∆° ch·∫ø k·ªπ thu·∫≠t** ƒë·ªÉ ph√°t hi·ªán v√† x·ª≠ l√Ω context overflow trong Gemini CLI.

---

## 1. B·ªêN C∆† CH·∫æ CH√çNH

```
1. OVERFLOW DETECTION (95% threshold)
   ‚Üí Ph√°t hi·ªán TR∆Ø·ªöC khi g·ª≠i API request

2. CHAT COMPRESSION (70/30 split)
   ‚Üí N√©n history b·∫±ng AI summarization

3. IDE CONTEXT DIFF (ch·ªâ g·ª≠i thay ƒë·ªïi)
   ‚Üí Ti·∫øt ki·ªám 10-100x tokens

4. MAX SESSION TURNS (default: 50)
   ‚Üí Gi·ªõi h·∫°n ƒë·ªô d√†i conversation
```

---

## 2. OVERFLOW DETECTION

### 2.1. Timing
**Khi n√†o check**: TR∆Ø·ªöC khi g·ª≠i request ƒë·∫øn API (trong `GeminiClient.sendMessageStream()`)

### 2.2. C√¥ng th·ª©c t√≠nh
```
estimatedTokens = JSON.stringify(request).length / 4

currentUsage = lastTurn.usage.totalTokenCount

remainingTokens = tokenLimit(model) - currentUsage

threshold = remainingTokens * 0.95

if (estimatedTokens > threshold):
    ‚Üí OVERFLOW!
```

### 2.3. Token Limits theo Model

| Model | Token Limit |
|-------|-------------|
| gemini-2.0-flash-thinking | 32,768 |
| gemini-2.0-* (others) | 1,000,000 |
| gemini-1.5-flash-002 | 1,000,000 |
| gemini-1.5-flash | 1,000,000 |
| gemini-1.5-pro-002 | 2,000,000 |
| gemini-1.5-pro | 2,000,000 |
| gemini-exp-* | 2,000,000 |
| Default fallback | 32,768 |

### 2.4. T·∫°i sao 95% threshold?

**L√Ω do**:
1. **Safety buffer** cho estimation error (4 chars = 1 token ch·ªâ l√† ∆∞·ªõc l∆∞·ª£ng)
2. **Response space** - Model c·∫ßn tokens ƒë·ªÉ generate response
3. **JSON overhead** - Structure th√™m tokens
4. **Better UX** - C·∫£nh b√°o tr∆∞·ªõc thay v√¨ API error

**V√≠ d·ª•**:
- Model: gemini-1.5-pro (2M limit)
- Current usage: 1,900,000 tokens
- Remaining: 100,000 tokens
- Threshold: 95,000 tokens
- New request estimate: 96,000 tokens
- **Result**: OVERFLOW! (96K > 95K)

---

## 3. COMPRESSION ALGORITHM

### 3.1. Strategy: 70/30 Split

**Nguy√™n l√Ω**:
- **Gi·ªØ nguy√™n** 30% messages g·∫ßn nh·∫•t (verbatim)
- **Summarize** 70% messages c≈© b·∫±ng AI
- **Thay th·∫ø** old messages b·∫±ng 1 summary message

### 3.2. Quy tr√¨nh

```
[1] T√≠nh split point
    numToPreserve = ceil(history.length * 0.3)
    numToCompress = history.length - numToPreserve

[2] Chia history
    oldMessages = history[0 : numToCompress]
    recentMessages = history[numToCompress : end]

[3] G·ªçi AI summarization
    summary = callAI("Summarize these messages: ...")

[4] Build compressed history
    compressedHistory = [
        { role: 'user', text: "[Summary]\n" + summary },
        ...recentMessages
    ]

[5] Replace history
    this.history = compressedHistory
```

### 3.3. Summarization Prompt Template

**Y√™u c·∫ßu v·ªõi AI**:
```
B·∫°n l√† chat history summarizer.

Summarize conversation n√†y, B·∫ÆT BU·ªòC gi·ªØ:
- Key facts v√† decisions
- Important context cho future messages
- Technical details v√† code snippets (quan tr·ªçng)
- User preferences v√† settings

C√ì TH·ªÇ b·ªè qua:
- Pleasantries (xin ch√†o, c·∫£m ∆°n, etc.)
- Redundant explanations
- Obvious/general knowledge

Format: Clear, structured, < 1000 tokens
```

### 3.4. Compression Example

**Before** (100 messages, ~180K tokens):
```
[
  {user: "Hi, help with React"},
  {model: "Sure! What issue?"},
  {user: "useEffect bug..."},
  {model: "[long explanation]"},
  ... (96 messages more) ...
]
```

**After** (31 messages, ~60K tokens):
```
[
  {user: "[Compressed Summary]
    User developing React app, encountered:
    1. useEffect bug ‚Üí fixed dependency array
    2. Added data fetching with custom hooks
    3. Project: React 18 + TypeScript
    User prefers: functional components, detailed explanations
  "},
  ... (30 recent messages - kept verbatim) ...
]
```

**Token savings**: 180K ‚Üí 60K = **67% reduction**

---

## 4. IDE CONTEXT DIFF

### 4.1. Problem

**Kh√¥ng c√≥ diff**:
```
Turn 1: G·ª≠i 20 open files = 50K tokens
Turn 2: G·ª≠i 20 open files = 50K tokens (l·∫°i!)
Turn 3: G·ª≠i 20 open files = 50K tokens (l·∫°i!)
‚Üí Waste: 150K tokens ch·ªâ ƒë·ªÉ g·ª≠i c√πng files
```

**C√≥ diff**:
```
Turn 1: G·ª≠i "Opened: 20 files" = 50K tokens
Turn 2: G·ª≠i "Modified: 1 file" = 500 tokens
Turn 3: G·ª≠i "Nothing changed" = 0 tokens
‚Üí Savings: 149.5K tokens (99% reduction!)
```

### 4.2. Diff Calculation

**So s√°nh v·ªõi previous turn**:

```
previousContext = lastTurn.ideContext
currentContext = getIdeContext()

diff = {
    openedFiles: current.files - previous.files,
    closedFiles: previous.files - current.files,
    modifiedFiles: files c√≥ same path nh∆∞ng different content,
    activeFileChanged: previous.activeFile != current.activeFile
}
```

**Ch·ªâ g·ª≠i diff** thay v√¨ to√†n b·ªô context!

### 4.3. Example

**Turn 1** (kh·ªüi t·∫°o):
```
IDE Context:
  openFiles: [App.tsx, Header.tsx, utils.ts]
  activeFile: App.tsx

‚Üí G·ª≠i: "Opened: App.tsx, Header.tsx, utils.ts
        Active: App.tsx"
```

**Turn 2** (user edit App.tsx):
```
IDE Context:
  openFiles: [App.tsx, Header.tsx, utils.ts]  // App.tsx modified
  activeFile: App.tsx

‚Üí G·ª≠i: "Modified: App.tsx (n·ªôi dung m·ªõi)"
```

**Turn 3** (user ƒë√≥ng Header.tsx, m·ªü New.tsx):
```
‚Üí G·ª≠i: "Closed: Header.tsx
        Opened: New.tsx"
```

### 4.4. Token Savings

| Scenario | Without Diff | With Diff | Savings |
|----------|--------------|-----------|---------|
| 20 files, no change | 50K | 0 | 100% |
| 20 files, 1 modified | 50K | 500 | 99% |
| 20 files, all new | 50K | 50K | 0% |

**Average savings**: 70-90%

---

## 5. CURATED HISTORY

### 5.1. Problem
Old IDE context trong history g√¢y nhi·ªÖu khi compression:
- File snapshots c≈© kh√¥ng c√≤n relevant
- TƒÉng k√≠ch th∆∞·ªõc history kh√¥ng c·∫ßn thi·∫øt
- L√†m gi·∫£m ch·∫•t l∆∞·ª£ng summarization

### 5.2. Solution: Filter tr∆∞·ªõc khi compress

**Curated History** = History ƒë√£ l·ªçc b·ªè old IDE context

```
L·ªçc b·ªè:
- Parts c√≥ "[IDE Context]" marker
- Old file snapshots
- System messages c≈©

Gi·ªØ l·∫°i:
- User/model text
- Tool calls v√† results
- Current IDE context (n·∫øu c√≥)
```

### 5.3. Impact

**Tr∆∞·ªõc khi curate**:
```
History size: 200 messages
Total tokens: 500K
IDE context: 200K tokens (40%)
```

**Sau khi curate**:
```
History size: 200 messages
Total tokens: 300K
IDE context: 0 tokens (removed!)
```

**Compression quality**: Better, v√¨ less noise

---

## 6. MAX SESSION TURNS LIMIT

### 6.1. Purpose
NgƒÉn conversations qu√° d√†i d√π kh√¥ng overflow:

**L√Ω do**:
1. **Quality degradation** - Conversations qu√° d√†i m·∫•t coherence
2. **Performance** - Processing large history ch·∫≠m
3. **Cost** - More tokens = more expensive
4. **Force refresh** - User ph·∫£i start fresh periodically

### 6.2. Configuration

**Default**: 50 turns (= 100 messages)

**Ki·ªÉm tra**:
```
turnCount = history.length / 2
if (turnCount >= maxSessionTurns):
    ‚Üí Emit MaxSessionTurnsExceeded event
    ‚Üí Suggest compression ho·∫∑c new session
```

### 6.3. User Options khi v∆∞·ª£t

1. **Compress** - N√©n history ƒë·ªÉ ti·∫øp t·ª•c
2. **New Session** - Clear history, start fresh
3. **Ignore** (n·∫øu config cho ph√©p) - Ti·∫øp t·ª•c d√π warning

---

## 7. COMPLETE OVERFLOW FLOW

```
submitQuery(text)
    ‚Üì
prepareQuery()
    ‚Üì
GeminiClient.sendMessageStream()
    ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ CHECKPOINT 1: Token Overflow    ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ estimate > remaining * 0.95?    ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ YES ‚Üí Emit ContextWillOverflow  ‚îÇ
    ‚îÇ       STOP, ƒë·ª£i user decision   ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ NO ‚Üí Continue                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ CHECKPOINT 2: Max Turns         ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ turnCount >= maxSessionTurns?   ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ YES ‚Üí Emit MaxTurnsExceeded     ‚îÇ
    ‚îÇ       Suggest compression       ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ NO ‚Üí Continue                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
                 Call API
```

### User Decision Dialog

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ö†Ô∏è Context Window Almost Full        ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ Current: 1.9M / 2M tokens (95%)      ‚îÇ
‚îÇ Remaining: 100K tokens               ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ Options:                              ‚îÇ
‚îÇ  1. [Compress] - N√©n history (recommended) ‚îÇ
‚îÇ  2. [New Session] - Start fresh      ‚îÇ
‚îÇ  3. [Cancel] - Kh√¥ng g·ª≠i message n√†y ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**N·∫øu ch·ªçn Compress**:
```
‚Üí Show "Compressing..."
‚Üí Run compression algorithm
‚Üí Show "Compressed: 180K ‚Üí 60K (saved 67%)"
‚Üí Retry original request
```

---

## 8. PERFORMANCE CONSIDERATIONS

### 8.1. Token Estimation Speed

**Current**: `JSON.stringify().length / 4`
- Speed: ~0.1ms cho 10KB content
- Accuracy: ¬±10%
- Good enough cho overflow detection

**Alternative**: Tokenization library (tiktoken)
- Speed: ~5-10ms cho 10KB
- Accuracy: 100%
- D√πng cho billing/analytics, kh√¥ng d√πng cho real-time check

### 8.2. Compression Latency

**Timings**:
- 50 messages: ~2-3 seconds
- 100 messages: ~4-6 seconds
- 200 messages: ~8-12 seconds

**UX**: Show progress indicator n·∫øu > 2 seconds

### 8.3. Diff Calculation Overhead

**Complexity**: O(n) where n = s·ªë open files

**Optimization**:
```
Thay v√¨:
  O(n¬≤) - Nested loops ƒë·ªÉ t√¨m modified files

D√πng:
  O(n) - Map lookup cho constant-time comparison
```

**Impact**: 10x faster v·ªõi 100+ open files

---

## 9. KEY TECHNICAL INSIGHTS

### 1. Proactive Detection
Check overflow **BEFORE** API call, kh√¥ng ph·∫£i react sau error

### 2. Multi-Layer Defense
- Overflow detection (95%)
- Max turns limit (50)
- IDE context diff (save 90%)
- Compression (save 67%)

### 3. User Control
User quy·∫øt ƒë·ªãnh compress hay new session, kh√¥ng t·ª± ƒë·ªông

### 4. Quality vs Size Tradeoff
- 70/30 split: Balance gi·ªØa compression ratio v√† context quality
- 95% threshold: Balance gi·ªØa safety v√† usability

### 5. Estimation vs Accuracy
- Fast estimation cho real-time check
- Accurate counting cho analytics
- Don't need perfect accuracy cho overflow detection

---

## üìö REFERENCES

### Files quan tr·ªçng:
- `client.ts:503-517` - Overflow detection
- `client.ts:731-859` - Compression algorithm
- `client.ts:295-461` - IDE context diff
- `tokenLimits.ts:8-58` - Model token limits

### Related Docs:
- **02_PROMPT_PROCESSING_FLOW.md** - Overflow check trong main flow
- **03_CONVERSATION_HISTORY.md** - History structure
- **05_UI_AND_STREAMING.md** - Compression UI

---

**C·∫≠p nh·∫≠t**: 2025-11-02
**Lo·∫°i**: M√¥ t·∫£ k·ªπ thu·∫≠t (technical description)
**Kh√¥ng bao g·ªìm**: Implementation code
